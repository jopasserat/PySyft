{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Asynchronous federated learning on MNIST\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers in an asynchronous way using [TrainConfig](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Federated%20Learning%20with%20TrainConfig/Introduction%20to%20TrainConfig.ipynb). We will use federated averaging to join the remotely trained models.\n",
    "\n",
    "Authors:\n",
    "- Silvia - GitHub [@midokura-silvia](https://github.com/midokura-silvia)\n",
    "- Marianne Monteiro - Twitter [@hereismari](https://twitter.com/hereismari) - Github [@mari-linhares\n",
    "](https://github.com/mari-linhares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning setup\n",
    "\n",
    "For a Federated Learning setup with TrainConfig we need different participants:\n",
    "\n",
    "* _Workers_ that own datasets.\n",
    "\n",
    "* An entity that knows the workers and the dataset name that lives in each worker. We'll call this a _scheduler_.\n",
    "\n",
    "Each worker is represented by two parts, a proxy local to the scheduler (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Start the websocket workers\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python start_websocket_servers.py\n",
    "```\n",
    "\n",
    "#### What's going on?\n",
    "\n",
    "The script will instantiate three workers, Alice, Bob and Charlie and prepare their local data. \n",
    "Each worker is set up to have a subset of the MNIST training dataset. \n",
    "Alice holds all images corresponding to the digits 0-3, \n",
    "Bob holds all images corresponding to the digits 4-6 and \n",
    "Charlie holds all images corresponding to the digits 7-9.\n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to see the code of the function that starts a worker\n",
    "# import run_websocket_server\n",
    "\n",
    "# print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing let's first need to import dependencies, setup needed arguments and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import run_websocket_client as rwc\n",
    "if torch.__version__>= \"1.0.2\":\n",
    "    raise ValueError(f\"This tutorial currently does not support torch versions >= 1.0.2, you have version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook torch\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, cuda=False, federate_after_n_batches=10, lr=0.1, save_model=False, seed=1, test_batch_size=128, training_rounds=40, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local proxies to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<WebsocketClientWorker id:alice #objects local:0 #objects remote: 1>, <WebsocketClientWorker id:bob #objects local:0 #objects remote: 1>, <WebsocketClientWorker id:charlie #objects local:0 #objects remote: 1>]\n"
     ]
    }
   ],
   "source": [
    "kwargs_websocket = {\"host\": \"localhost\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket)\n",
    "charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "testing = WebsocketClientWorker(id=\"testing\", port=8780, **kwargs_websocket)\n",
    "\n",
    "worker_instances = [alice, bob, charlie]\n",
    "print(worker_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's instantiate the machine learning model. It is a small neural network with 2 convolutional and two fully connected layers. \n",
    "It uses ReLU activations and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
      "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
      "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
      "        self.fc2 = nn.Linear(500, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = F.relu(self.conv1(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = F.relu(self.conv2(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = x.view(-1, 4 * 4 * 50)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return F.log_softmax(x, dim=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "The training data lives in each of the devices, but fefore starting the training, let's load the MNIST test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "(data, target) = test_loader.__iter__().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the model serializable\n",
    "\n",
    "In order to send the model to the workers we need the model to be serializable, for this we use [`jit`](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the training\n",
    "\n",
    "Now we are ready to start the federated training. We will perform training over a given number of batches separately on each worker and then calculate the federated average of the resulting model.\n",
    "\n",
    "Every 10th training round we will evaluate the performance of the models returned by the workers and of the model obtained by federated averaging. \n",
    "\n",
    "The performance will be given both as the accuracy (ratio of correct predictions) and as the histograms of predicted digits. This is of interest, as each worker only owns a subset of the digits. Therefore, in the beginning each worker will only predict their numbers and only know about the other numbers via the federated averaging process.\n",
    "\n",
    "The training is done in an asynchronous manner. This means that the scheduler just tell the workers to train and does not block to wait for the result of the training before talking to the next worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the training are given in the arguments. \n",
    "Each worker will train on a given number of batches, given by the value of federate_after_n_batches.\n",
    "The training batch size and learning rate are also configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federate_after_n_batches: 10\n",
      "Batch size: 32\n",
      "Initial learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:24:17 - Starting training round 1/40\n",
      "16:24:17 - Training round 1, calling fit on worker: alice\n",
      "16:24:17 - Training round 1, calling fit on worker: bob\n",
      "16:24:17 - Training round 1, calling fit on worker: charlie\n",
      "16:24:18 - Training round: 1, worker: alice, avg_loss: tensor(1.2938, grad_fn=<MeanBackward1>)\n",
      "16:24:19 - Training round: 1, worker: charlie, avg_loss: tensor(1.7821, grad_fn=<MeanBackward1>)\n",
      "16:24:20 - Training round: 1, worker: bob, avg_loss: tensor(1.2307, grad_fn=<MeanBackward1>)\n",
      "16:24:23 - alice: Prediction hist.: [1.835e+03 0.000e+00 6.000e+00 8.159e+03 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00]\n",
      "16:24:23 - alice: Percentage numbers 0-3: 1.0\n",
      "16:24:23 - alice: Percentage numbers 4-6: 0.0\n",
      "16:24:23 - alice: Percentage numbers 7-9: 0.0\n",
      "16:24:23 - alice: Test set: Average loss: 0.0237, Accuracy: 1958/10000 (19.58)\n",
      "16:24:25 - bob: Prediction hist.: [   0.    0.    0.    0. 8778. 1222.    0.    0.    0.    0.]\n",
      "16:24:25 - bob: Percentage numbers 0-3: 0.0\n",
      "16:24:25 - bob: Percentage numbers 4-6: 1.0\n",
      "16:24:25 - bob: Percentage numbers 7-9: 0.0\n",
      "16:24:25 - bob: Test set: Average loss: 0.0266, Accuracy: 1431/10000 (14.31)\n",
      "16:24:27 - charlie: Prediction hist.: [    0.     0.     0.     0.     0.     0.     0.     0.     0. 10000.]\n",
      "16:24:27 - charlie: Percentage numbers 0-3: 0.0\n",
      "16:24:27 - charlie: Percentage numbers 4-6: 0.0\n",
      "16:24:27 - charlie: Percentage numbers 7-9: 1.0\n",
      "16:24:27 - charlie: Test set: Average loss: 0.0223, Accuracy: 1009/10000 (10.09)\n",
      "16:24:29 - Target histogram: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
      "16:24:29 - Federated model: Prediction hist.: [3.009e+03 3.000e+00 0.000e+00 1.922e+03 3.381e+03 0.000e+00 0.000e+00\n",
      " 1.161e+03 3.000e+00 5.210e+02]\n",
      "16:24:29 - Federated model: Percentage numbers 0-3: 0.4934\n",
      "16:24:29 - Federated model: Percentage numbers 4-6: 0.3381\n",
      "16:24:29 - Federated model: Percentage numbers 7-9: 0.1685\n",
      "16:24:29 - Federated model: Test set: Average loss: 0.0182, Accuracy: 933/10000 (9.33)\n",
      "16:24:29 - Starting training round 2/40\n",
      "16:24:29 - Training round 2, calling fit on worker: alice\n",
      "16:24:29 - Training round 2, calling fit on worker: bob\n",
      "16:24:29 - Training round 2, calling fit on worker: charlie\n",
      "16:24:30 - Training round: 2, worker: alice, avg_loss: tensor(1.1848, grad_fn=<MeanBackward1>)\n",
      "16:24:31 - Training round: 2, worker: charlie, avg_loss: tensor(1.2845, grad_fn=<MeanBackward1>)\n",
      "16:24:32 - Training round: 2, worker: bob, avg_loss: tensor(1.6721, grad_fn=<MeanBackward1>)\n",
      "16:24:34 - Starting training round 3/40\n",
      "16:24:34 - Training round 3, calling fit on worker: alice\n",
      "16:24:34 - Training round 3, calling fit on worker: bob\n",
      "16:24:34 - Training round 3, calling fit on worker: charlie\n",
      "16:24:34 - Training round: 3, worker: alice, avg_loss: tensor(0.5319, grad_fn=<MeanBackward1>)\n",
      "16:24:36 - Training round: 3, worker: charlie, avg_loss: tensor(1.6169, grad_fn=<MeanBackward1>)\n",
      "16:24:37 - Training round: 3, worker: bob, avg_loss: tensor(0.5329, grad_fn=<MeanBackward1>)\n",
      "16:24:38 - Starting training round 4/40\n",
      "16:24:38 - Training round 4, calling fit on worker: alice\n",
      "16:24:38 - Training round 4, calling fit on worker: bob\n",
      "16:24:38 - Training round 4, calling fit on worker: charlie\n",
      "16:24:39 - Training round: 4, worker: bob, avg_loss: tensor(0.5487, grad_fn=<MeanBackward1>)\n",
      "16:24:40 - Training round: 4, worker: alice, avg_loss: tensor(0.6643, grad_fn=<MeanBackward1>)\n",
      "16:24:42 - Training round: 4, worker: charlie, avg_loss: tensor(1.1311, grad_fn=<MeanBackward1>)\n",
      "16:24:43 - Starting training round 5/40\n",
      "16:24:43 - Training round 5, calling fit on worker: alice\n",
      "16:24:43 - Training round 5, calling fit on worker: bob\n",
      "16:24:43 - Training round 5, calling fit on worker: charlie\n",
      "16:24:43 - Training round: 5, worker: alice, avg_loss: tensor(0.3013, grad_fn=<MeanBackward1>)\n",
      "16:24:45 - Training round: 5, worker: bob, avg_loss: tensor(0.2724, grad_fn=<MeanBackward1>)\n",
      "16:24:46 - Training round: 5, worker: charlie, avg_loss: tensor(0.2947, grad_fn=<MeanBackward1>)\n",
      "16:24:47 - Starting training round 6/40\n",
      "16:24:47 - Training round 6, calling fit on worker: alice\n",
      "16:24:47 - Training round 6, calling fit on worker: bob\n",
      "16:24:47 - Training round 6, calling fit on worker: charlie\n",
      "16:24:48 - Training round: 6, worker: bob, avg_loss: tensor(0.1260, grad_fn=<MeanBackward1>)\n",
      "16:24:49 - Training round: 6, worker: alice, avg_loss: tensor(0.1515, grad_fn=<MeanBackward1>)\n",
      "16:24:50 - Training round: 6, worker: charlie, avg_loss: tensor(0.2499, grad_fn=<MeanBackward1>)\n",
      "16:24:51 - Starting training round 7/40\n",
      "16:24:51 - Training round 7, calling fit on worker: alice\n",
      "16:24:52 - Training round 7, calling fit on worker: bob\n",
      "16:24:52 - Training round 7, calling fit on worker: charlie\n",
      "16:24:52 - Training round: 7, worker: charlie, avg_loss: tensor(0.1278, grad_fn=<MeanBackward1>)\n",
      "16:24:53 - Training round: 7, worker: bob, avg_loss: tensor(0.6136, grad_fn=<MeanBackward1>)\n",
      "16:24:55 - Training round: 7, worker: alice, avg_loss: tensor(0.7596, grad_fn=<MeanBackward1>)\n",
      "16:24:56 - Starting training round 8/40\n",
      "16:24:56 - Training round 8, calling fit on worker: alice\n",
      "16:24:56 - Training round 8, calling fit on worker: bob\n",
      "16:24:56 - Training round 8, calling fit on worker: charlie\n",
      "16:24:56 - Training round: 8, worker: alice, avg_loss: tensor(0.0130, grad_fn=<MeanBackward1>)\n",
      "16:24:58 - Training round: 8, worker: bob, avg_loss: tensor(0.0145, grad_fn=<MeanBackward1>)\n",
      "16:24:59 - Training round: 8, worker: charlie, avg_loss: tensor(0.1290, grad_fn=<MeanBackward1>)\n",
      "16:25:00 - Starting training round 9/40\n",
      "16:25:00 - Training round 9, calling fit on worker: alice\n",
      "16:25:00 - Training round 9, calling fit on worker: bob\n",
      "16:25:00 - Training round 9, calling fit on worker: charlie\n",
      "16:25:01 - Training round: 9, worker: bob, avg_loss: tensor(0.0492, grad_fn=<MeanBackward1>)\n",
      "16:25:02 - Training round: 9, worker: alice, avg_loss: tensor(0.0879, grad_fn=<MeanBackward1>)\n",
      "16:25:03 - Training round: 9, worker: charlie, avg_loss: tensor(0.3895, grad_fn=<MeanBackward1>)\n",
      "16:25:05 - Starting training round 10/40\n",
      "16:25:05 - Training round 10, calling fit on worker: alice\n",
      "16:25:05 - Training round 10, calling fit on worker: bob\n",
      "16:25:05 - Training round 10, calling fit on worker: charlie\n",
      "16:25:05 - Training round: 10, worker: bob, avg_loss: tensor(0.0069, grad_fn=<MeanBackward1>)\n",
      "16:25:07 - Training round: 10, worker: alice, avg_loss: tensor(0.1592, grad_fn=<MeanBackward1>)\n",
      "16:25:08 - Training round: 10, worker: charlie, avg_loss: tensor(0.2736, grad_fn=<MeanBackward1>)\n",
      "16:25:09 - Starting training round 11/40\n",
      "16:25:10 - Training round 11, calling fit on worker: alice\n",
      "16:25:10 - Training round 11, calling fit on worker: bob\n",
      "16:25:10 - Training round 11, calling fit on worker: charlie\n",
      "16:25:10 - Training round: 11, worker: charlie, avg_loss: tensor(0.1572, grad_fn=<MeanBackward1>)\n",
      "16:25:12 - Training round: 11, worker: bob, avg_loss: tensor(0.0509, grad_fn=<MeanBackward1>)\n",
      "16:25:13 - Training round: 11, worker: alice, avg_loss: tensor(0.0457, grad_fn=<MeanBackward1>)\n",
      "16:25:16 - alice: Prediction hist.: [1926. 1527. 1727. 2797.  521.   18.  200.  652.   60.  572.]\n",
      "16:25:16 - alice: Percentage numbers 0-3: 0.7977\n",
      "16:25:16 - alice: Percentage numbers 4-6: 0.0739\n",
      "16:25:16 - alice: Percentage numbers 7-9: 0.1284\n",
      "16:25:16 - alice: Test set: Average loss: 0.0108, Accuracy: 5945/10000 (59.45)\n",
      "16:25:19 - bob: Prediction hist.: [  12.  821.  427.    0. 2130. 3844. 2080.  686.    0.    0.]\n",
      "16:25:19 - bob: Percentage numbers 0-3: 0.126\n",
      "16:25:19 - bob: Percentage numbers 4-6: 0.8054\n",
      "16:25:19 - bob: Percentage numbers 7-9: 0.0686\n",
      "16:25:19 - bob: Test set: Average loss: 0.0198, Accuracy: 4674/10000 (46.74)\n",
      "16:25:21 - charlie: Prediction hist.: [  70.    9.  123.    0.    0.    0.   23. 1009. 6179. 2587.]\n",
      "16:25:21 - charlie: Percentage numbers 0-3: 0.0202\n",
      "16:25:21 - charlie: Percentage numbers 4-6: 0.0023\n",
      "16:25:21 - charlie: Percentage numbers 7-9: 0.9775\n",
      "16:25:21 - charlie: Test set: Average loss: 0.0204, Accuracy: 3041/10000 (30.41)\n",
      "16:25:23 - Target histogram: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
      "16:25:23 - Federated model: Prediction hist.: [1031. 1201. 1027.  831.  908.  893. 1094. 1113.  826. 1076.]\n",
      "16:25:23 - Federated model: Percentage numbers 0-3: 0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:25:23 - Federated model: Percentage numbers 4-6: 0.2895\n",
      "16:25:23 - Federated model: Percentage numbers 7-9: 0.3015\n",
      "16:25:23 - Federated model: Test set: Average loss: 0.0034, Accuracy: 8902/10000 (89.02)\n",
      "16:25:23 - Starting training round 12/40\n",
      "16:25:23 - Training round 12, calling fit on worker: alice\n",
      "16:25:23 - Training round 12, calling fit on worker: bob\n",
      "16:25:23 - Training round 12, calling fit on worker: charlie\n",
      "16:25:23 - Training round: 12, worker: bob, avg_loss: tensor(0.0700, grad_fn=<MeanBackward1>)\n",
      "16:25:25 - Training round: 12, worker: charlie, avg_loss: tensor(0.1274, grad_fn=<MeanBackward1>)\n",
      "16:25:26 - Training round: 12, worker: alice, avg_loss: tensor(0.0524, grad_fn=<MeanBackward1>)\n",
      "16:25:27 - Starting training round 13/40\n",
      "16:25:27 - Training round 13, calling fit on worker: alice\n",
      "16:25:28 - Training round 13, calling fit on worker: bob\n",
      "16:25:28 - Training round 13, calling fit on worker: charlie\n",
      "16:25:28 - Training round: 13, worker: alice, avg_loss: tensor(0.0165, grad_fn=<MeanBackward1>)\n",
      "16:25:29 - Training round: 13, worker: bob, avg_loss: tensor(0.0537, grad_fn=<MeanBackward1>)\n",
      "16:25:31 - Training round: 13, worker: charlie, avg_loss: tensor(0.2870, grad_fn=<MeanBackward1>)\n",
      "16:25:32 - Starting training round 14/40\n",
      "16:25:32 - Training round 14, calling fit on worker: alice\n",
      "16:25:32 - Training round 14, calling fit on worker: bob\n",
      "16:25:32 - Training round 14, calling fit on worker: charlie\n",
      "16:25:33 - Training round: 14, worker: bob, avg_loss: tensor(0.0621, grad_fn=<MeanBackward1>)\n",
      "16:25:34 - Training round: 14, worker: alice, avg_loss: tensor(0.1262, grad_fn=<MeanBackward1>)\n",
      "16:25:35 - Training round: 14, worker: charlie, avg_loss: tensor(0.3237, grad_fn=<MeanBackward1>)\n",
      "16:25:37 - Starting training round 15/40\n",
      "16:25:37 - Training round 15, calling fit on worker: alice\n",
      "16:25:37 - Training round 15, calling fit on worker: bob\n",
      "16:25:37 - Training round 15, calling fit on worker: charlie\n",
      "16:25:38 - Training round: 15, worker: charlie, avg_loss: tensor(0.0905, grad_fn=<MeanBackward1>)\n",
      "16:25:39 - Training round: 15, worker: alice, avg_loss: tensor(0.0656, grad_fn=<MeanBackward1>)\n",
      "16:25:40 - Training round: 15, worker: bob, avg_loss: tensor(0.0197, grad_fn=<MeanBackward1>)\n",
      "16:25:42 - Starting training round 16/40\n",
      "16:25:42 - Training round 16, calling fit on worker: alice\n",
      "16:25:42 - Training round 16, calling fit on worker: bob\n",
      "16:25:42 - Training round 16, calling fit on worker: charlie\n",
      "16:25:43 - Training round: 16, worker: alice, avg_loss: tensor(0.0150, grad_fn=<MeanBackward1>)\n",
      "16:25:44 - Training round: 16, worker: charlie, avg_loss: tensor(0.0964, grad_fn=<MeanBackward1>)\n",
      "16:25:46 - Training round: 16, worker: bob, avg_loss: tensor(0.0109, grad_fn=<MeanBackward1>)\n",
      "16:25:47 - Starting training round 17/40\n",
      "16:25:47 - Training round 17, calling fit on worker: alice\n",
      "16:25:47 - Training round 17, calling fit on worker: bob\n",
      "16:25:47 - Training round 17, calling fit on worker: charlie\n",
      "16:25:48 - Training round: 17, worker: bob, avg_loss: tensor(0.0118, grad_fn=<MeanBackward1>)\n",
      "16:25:49 - Training round: 17, worker: alice, avg_loss: tensor(0.0107, grad_fn=<MeanBackward1>)\n",
      "16:25:50 - Training round: 17, worker: charlie, avg_loss: tensor(0.1356, grad_fn=<MeanBackward1>)\n",
      "16:25:52 - Starting training round 18/40\n",
      "16:25:52 - Training round 18, calling fit on worker: alice\n",
      "16:25:52 - Training round 18, calling fit on worker: bob\n",
      "16:25:52 - Training round 18, calling fit on worker: charlie\n",
      "16:25:53 - Training round: 18, worker: charlie, avg_loss: tensor(0.1343, grad_fn=<MeanBackward1>)\n",
      "16:25:54 - Training round: 18, worker: bob, avg_loss: tensor(0.0533, grad_fn=<MeanBackward1>)\n",
      "16:25:55 - Training round: 18, worker: alice, avg_loss: tensor(0.0614, grad_fn=<MeanBackward1>)\n",
      "16:25:57 - Starting training round 19/40\n",
      "16:25:57 - Training round 19, calling fit on worker: alice\n",
      "16:25:57 - Training round 19, calling fit on worker: bob\n",
      "16:25:57 - Training round 19, calling fit on worker: charlie\n",
      "16:25:58 - Training round: 19, worker: bob, avg_loss: tensor(0.0166, grad_fn=<MeanBackward1>)\n",
      "16:25:59 - Training round: 19, worker: charlie, avg_loss: tensor(0.1668, grad_fn=<MeanBackward1>)\n",
      "16:26:01 - Training round: 19, worker: alice, avg_loss: tensor(0.1397, grad_fn=<MeanBackward1>)\n",
      "16:26:02 - Starting training round 20/40\n",
      "16:26:02 - Training round 20, calling fit on worker: alice\n",
      "16:26:02 - Training round 20, calling fit on worker: bob\n",
      "16:26:02 - Training round 20, calling fit on worker: charlie\n",
      "16:26:03 - Training round: 20, worker: charlie, avg_loss: tensor(0.5055, grad_fn=<MeanBackward1>)\n",
      "16:26:04 - Training round: 20, worker: bob, avg_loss: tensor(0.0177, grad_fn=<MeanBackward1>)\n",
      "16:26:06 - Training round: 20, worker: alice, avg_loss: tensor(0.0573, grad_fn=<MeanBackward1>)\n",
      "16:26:07 - Starting training round 21/40\n",
      "16:26:07 - Training round 21, calling fit on worker: alice\n",
      "16:26:07 - Training round 21, calling fit on worker: bob\n",
      "16:26:07 - Training round 21, calling fit on worker: charlie\n",
      "16:26:08 - Training round: 21, worker: charlie, avg_loss: tensor(0.0296, grad_fn=<MeanBackward1>)\n",
      "16:26:09 - Training round: 21, worker: bob, avg_loss: tensor(0.0081, grad_fn=<MeanBackward1>)\n",
      "16:26:10 - Training round: 21, worker: alice, avg_loss: tensor(0.1821, grad_fn=<MeanBackward1>)\n",
      "16:26:14 - alice: Prediction hist.: [1831. 1560.  994. 2684.  914.  295.  253.  790.  257.  422.]\n",
      "16:26:14 - alice: Percentage numbers 0-3: 0.7069\n",
      "16:26:14 - alice: Percentage numbers 4-6: 0.1462\n",
      "16:26:14 - alice: Percentage numbers 7-9: 0.1469\n",
      "16:26:14 - alice: Test set: Average loss: 0.0075, Accuracy: 6817/10000 (68.17)\n",
      "16:26:16 - bob: Prediction hist.: [  61. 1091.  762.  364. 1951. 2733. 1881.  952.  205.    0.]\n",
      "16:26:16 - bob: Percentage numbers 0-3: 0.2278\n",
      "16:26:16 - bob: Percentage numbers 4-6: 0.6565\n",
      "16:26:16 - bob: Percentage numbers 7-9: 0.1157\n",
      "16:26:16 - bob: Test set: Average loss: 0.0098, Accuracy: 6142/10000 (61.42)\n",
      "16:26:18 - charlie: Prediction hist.: [3.910e+02 5.030e+02 3.180e+02 2.660e+02 3.000e+00 6.500e+01 6.300e+01\n",
      " 1.138e+03 5.248e+03 2.005e+03]\n",
      "16:26:18 - charlie: Percentage numbers 0-3: 0.1478\n",
      "16:26:18 - charlie: Percentage numbers 4-6: 0.0131\n",
      "16:26:18 - charlie: Percentage numbers 7-9: 0.8391\n",
      "16:26:18 - charlie: Test set: Average loss: 0.0133, Accuracy: 4513/10000 (45.13)\n",
      "16:26:20 - Target histogram: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
      "16:26:20 - Federated model: Prediction hist.: [1009. 1153.  927. 1007. 1011.  949.  961. 1038. 1041.  904.]\n",
      "16:26:20 - Federated model: Percentage numbers 0-3: 0.4096\n",
      "16:26:20 - Federated model: Percentage numbers 4-6: 0.2921\n",
      "16:26:20 - Federated model: Percentage numbers 7-9: 0.2983\n",
      "16:26:20 - Federated model: Test set: Average loss: 0.0018, Accuracy: 9373/10000 (93.73)\n",
      "16:26:20 - Starting training round 22/40\n",
      "16:26:20 - Training round 22, calling fit on worker: alice\n",
      "16:26:21 - Training round 22, calling fit on worker: bob\n",
      "16:26:21 - Training round 22, calling fit on worker: charlie\n",
      "16:26:21 - Training round: 22, worker: bob, avg_loss: tensor(0.0439, grad_fn=<MeanBackward1>)\n",
      "16:26:23 - Training round: 22, worker: charlie, avg_loss: tensor(0.0435, grad_fn=<MeanBackward1>)\n",
      "16:26:24 - Training round: 22, worker: alice, avg_loss: tensor(0.0248, grad_fn=<MeanBackward1>)\n",
      "16:26:25 - Starting training round 23/40\n",
      "16:26:25 - Training round 23, calling fit on worker: alice\n",
      "16:26:25 - Training round 23, calling fit on worker: bob\n",
      "16:26:25 - Training round 23, calling fit on worker: charlie\n",
      "16:26:26 - Training round: 23, worker: alice, avg_loss: tensor(0.1216, grad_fn=<MeanBackward1>)\n",
      "16:26:28 - Training round: 23, worker: bob, avg_loss: tensor(0.0183, grad_fn=<MeanBackward1>)\n",
      "16:26:29 - Training round: 23, worker: charlie, avg_loss: tensor(0.0852, grad_fn=<MeanBackward1>)\n",
      "16:26:30 - Starting training round 24/40\n",
      "16:26:30 - Training round 24, calling fit on worker: alice\n",
      "16:26:30 - Training round 24, calling fit on worker: bob\n",
      "16:26:30 - Training round 24, calling fit on worker: charlie\n",
      "16:26:31 - Training round: 24, worker: charlie, avg_loss: tensor(0.0282, grad_fn=<MeanBackward1>)\n",
      "16:26:32 - Training round: 24, worker: bob, avg_loss: tensor(0.0481, grad_fn=<MeanBackward1>)\n",
      "16:26:34 - Training round: 24, worker: alice, avg_loss: tensor(0.1253, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:26:35 - Starting training round 25/40\n",
      "16:26:35 - Training round 25, calling fit on worker: alice\n",
      "16:26:35 - Training round 25, calling fit on worker: bob\n",
      "16:26:35 - Training round 25, calling fit on worker: charlie\n",
      "16:26:36 - Training round: 25, worker: charlie, avg_loss: tensor(0.1051, grad_fn=<MeanBackward1>)\n",
      "16:26:38 - Training round: 25, worker: bob, avg_loss: tensor(0.0088, grad_fn=<MeanBackward1>)\n",
      "16:26:39 - Training round: 25, worker: alice, avg_loss: tensor(0.1772, grad_fn=<MeanBackward1>)\n",
      "16:26:40 - Starting training round 26/40\n",
      "16:26:40 - Training round 26, calling fit on worker: alice\n",
      "16:26:41 - Training round 26, calling fit on worker: bob\n",
      "16:26:41 - Training round 26, calling fit on worker: charlie\n",
      "16:26:41 - Training round: 26, worker: bob, avg_loss: tensor(0.0277, grad_fn=<MeanBackward1>)\n",
      "16:26:43 - Training round: 26, worker: charlie, avg_loss: tensor(0.1580, grad_fn=<MeanBackward1>)\n",
      "16:26:44 - Training round: 26, worker: alice, avg_loss: tensor(0.0437, grad_fn=<MeanBackward1>)\n",
      "16:26:45 - Starting training round 27/40\n",
      "16:26:45 - Training round 27, calling fit on worker: alice\n",
      "16:26:45 - Training round 27, calling fit on worker: bob\n",
      "16:26:45 - Training round 27, calling fit on worker: charlie\n",
      "16:26:46 - Training round: 27, worker: charlie, avg_loss: tensor(0.1154, grad_fn=<MeanBackward1>)\n",
      "16:26:47 - Training round: 27, worker: alice, avg_loss: tensor(0.1119, grad_fn=<MeanBackward1>)\n",
      "16:26:49 - Training round: 27, worker: bob, avg_loss: tensor(0.0094, grad_fn=<MeanBackward1>)\n",
      "16:26:50 - Starting training round 28/40\n",
      "16:26:50 - Training round 28, calling fit on worker: alice\n",
      "16:26:50 - Training round 28, calling fit on worker: bob\n",
      "16:26:50 - Training round 28, calling fit on worker: charlie\n",
      "16:26:51 - Training round: 28, worker: bob, avg_loss: tensor(0.0382, grad_fn=<MeanBackward1>)\n",
      "16:26:52 - Training round: 28, worker: alice, avg_loss: tensor(0.1818, grad_fn=<MeanBackward1>)\n",
      "16:26:54 - Training round: 28, worker: charlie, avg_loss: tensor(0.1181, grad_fn=<MeanBackward1>)\n",
      "16:26:55 - Starting training round 29/40\n",
      "16:26:55 - Training round 29, calling fit on worker: alice\n",
      "16:26:55 - Training round 29, calling fit on worker: bob\n",
      "16:26:55 - Training round 29, calling fit on worker: charlie\n",
      "16:26:55 - Training round: 29, worker: bob, avg_loss: tensor(0.0118, grad_fn=<MeanBackward1>)\n",
      "16:26:57 - Training round: 29, worker: charlie, avg_loss: tensor(0.1577, grad_fn=<MeanBackward1>)\n",
      "16:26:58 - Training round: 29, worker: alice, avg_loss: tensor(0.0268, grad_fn=<MeanBackward1>)\n",
      "16:26:59 - Starting training round 30/40\n",
      "16:26:59 - Training round 30, calling fit on worker: alice\n",
      "16:26:59 - Training round 30, calling fit on worker: bob\n",
      "16:26:59 - Training round 30, calling fit on worker: charlie\n",
      "16:27:00 - Training round: 30, worker: bob, avg_loss: tensor(0.0385, grad_fn=<MeanBackward1>)\n",
      "16:27:01 - Training round: 30, worker: alice, avg_loss: tensor(0.0094, grad_fn=<MeanBackward1>)\n",
      "16:27:03 - Training round: 30, worker: charlie, avg_loss: tensor(0.0201, grad_fn=<MeanBackward1>)\n",
      "16:27:04 - Starting training round 31/40\n",
      "16:27:04 - Training round 31, calling fit on worker: alice\n",
      "16:27:04 - Training round 31, calling fit on worker: bob\n",
      "16:27:04 - Training round 31, calling fit on worker: charlie\n",
      "16:27:05 - Training round: 31, worker: bob, avg_loss: tensor(0.0097, grad_fn=<MeanBackward1>)\n",
      "16:27:06 - Training round: 31, worker: charlie, avg_loss: tensor(0.0068, grad_fn=<MeanBackward1>)\n",
      "16:27:07 - Training round: 31, worker: alice, avg_loss: tensor(0.2159, grad_fn=<MeanBackward1>)\n",
      "16:27:11 - alice: Prediction hist.: [ 952. 1157. 2017. 2070.  936.  407.  558.  863.  329.  711.]\n",
      "16:27:11 - alice: Percentage numbers 0-3: 0.6196\n",
      "16:27:11 - alice: Percentage numbers 4-6: 0.1901\n",
      "16:27:11 - alice: Percentage numbers 7-9: 0.1903\n",
      "16:27:11 - alice: Test set: Average loss: 0.0050, Accuracy: 7776/10000 (77.76)\n",
      "16:27:13 - bob: Prediction hist.: [ 762. 1077.  961.  574. 1830. 2025. 1339.  997.  405.   30.]\n",
      "16:27:13 - bob: Percentage numbers 0-3: 0.3374\n",
      "16:27:13 - bob: Percentage numbers 4-6: 0.5194\n",
      "16:27:13 - bob: Percentage numbers 7-9: 0.1432\n",
      "16:27:13 - bob: Test set: Average loss: 0.0058, Accuracy: 7490/10000 (74.90)\n",
      "16:27:15 - charlie: Prediction hist.: [ 857.  900.  603.  613.   59.  440.  782. 1260. 2200. 2286.]\n",
      "16:27:15 - charlie: Percentage numbers 0-3: 0.2973\n",
      "16:27:15 - charlie: Percentage numbers 4-6: 0.1281\n",
      "16:27:15 - charlie: Percentage numbers 7-9: 0.5746\n",
      "16:27:15 - charlie: Test set: Average loss: 0.0065, Accuracy: 7146/10000 (71.46)\n",
      "16:27:17 - Target histogram: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
      "16:27:17 - Federated model: Prediction hist.: [ 995. 1128.  979.  968. 1043.  905.  954. 1064. 1024.  940.]\n",
      "16:27:17 - Federated model: Percentage numbers 0-3: 0.407\n",
      "16:27:17 - Federated model: Percentage numbers 4-6: 0.2902\n",
      "16:27:17 - Federated model: Percentage numbers 7-9: 0.3028\n",
      "16:27:17 - Federated model: Test set: Average loss: 0.0013, Accuracy: 9536/10000 (95.36)\n",
      "16:27:17 - Starting training round 32/40\n",
      "16:27:17 - Training round 32, calling fit on worker: alice\n",
      "16:27:17 - Training round 32, calling fit on worker: bob\n",
      "16:27:17 - Training round 32, calling fit on worker: charlie\n",
      "16:27:18 - Training round: 32, worker: charlie, avg_loss: tensor(0.0531, grad_fn=<MeanBackward1>)\n",
      "16:27:19 - Training round: 32, worker: bob, avg_loss: tensor(0.0115, grad_fn=<MeanBackward1>)\n",
      "16:27:20 - Training round: 32, worker: alice, avg_loss: tensor(0.0524, grad_fn=<MeanBackward1>)\n",
      "16:27:22 - Starting training round 33/40\n",
      "16:27:22 - Training round 33, calling fit on worker: alice\n",
      "16:27:22 - Training round 33, calling fit on worker: bob\n",
      "16:27:22 - Training round 33, calling fit on worker: charlie\n",
      "16:27:22 - Training round: 33, worker: charlie, avg_loss: tensor(0.0964, grad_fn=<MeanBackward1>)\n",
      "16:27:24 - Training round: 33, worker: bob, avg_loss: tensor(0.0124, grad_fn=<MeanBackward1>)\n",
      "16:27:25 - Training round: 33, worker: alice, avg_loss: tensor(0.1565, grad_fn=<MeanBackward1>)\n",
      "16:27:26 - Starting training round 34/40\n",
      "16:27:26 - Training round 34, calling fit on worker: alice\n",
      "16:27:26 - Training round 34, calling fit on worker: bob\n",
      "16:27:26 - Training round 34, calling fit on worker: charlie\n",
      "16:27:27 - Training round: 34, worker: alice, avg_loss: tensor(0.0146, grad_fn=<MeanBackward1>)\n",
      "16:27:28 - Training round: 34, worker: bob, avg_loss: tensor(0.0028, grad_fn=<MeanBackward1>)\n",
      "16:27:30 - Training round: 34, worker: charlie, avg_loss: tensor(0.1188, grad_fn=<MeanBackward1>)\n",
      "16:27:31 - Starting training round 35/40\n",
      "16:27:31 - Training round 35, calling fit on worker: alice\n",
      "16:27:31 - Training round 35, calling fit on worker: bob\n",
      "16:27:31 - Training round 35, calling fit on worker: charlie\n",
      "16:27:31 - Training round: 35, worker: bob, avg_loss: tensor(0.0083, grad_fn=<MeanBackward1>)\n",
      "16:27:33 - Training round: 35, worker: alice, avg_loss: tensor(0.0319, grad_fn=<MeanBackward1>)\n",
      "16:27:34 - Training round: 35, worker: charlie, avg_loss: tensor(0.0280, grad_fn=<MeanBackward1>)\n",
      "16:27:35 - Starting training round 36/40\n",
      "16:27:35 - Training round 36, calling fit on worker: alice\n",
      "16:27:35 - Training round 36, calling fit on worker: bob\n",
      "16:27:36 - Training round 36, calling fit on worker: charlie\n",
      "16:27:36 - Training round: 36, worker: alice, avg_loss: tensor(0.0729, grad_fn=<MeanBackward1>)\n",
      "16:27:37 - Training round: 36, worker: bob, avg_loss: tensor(0.0369, grad_fn=<MeanBackward1>)\n",
      "16:27:39 - Training round: 36, worker: charlie, avg_loss: tensor(0.0199, grad_fn=<MeanBackward1>)\n",
      "16:27:40 - Starting training round 37/40\n",
      "16:27:40 - Training round 37, calling fit on worker: alice\n",
      "16:27:40 - Training round 37, calling fit on worker: bob\n",
      "16:27:40 - Training round 37, calling fit on worker: charlie\n",
      "16:27:41 - Training round: 37, worker: charlie, avg_loss: tensor(0.0107, grad_fn=<MeanBackward1>)\n",
      "16:27:42 - Training round: 37, worker: alice, avg_loss: tensor(0.0067, grad_fn=<MeanBackward1>)\n",
      "16:27:43 - Training round: 37, worker: bob, avg_loss: tensor(0.0118, grad_fn=<MeanBackward1>)\n",
      "16:27:44 - Starting training round 38/40\n",
      "16:27:45 - Training round 38, calling fit on worker: alice\n",
      "16:27:45 - Training round 38, calling fit on worker: bob\n",
      "16:27:45 - Training round 38, calling fit on worker: charlie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:27:45 - Training round: 38, worker: alice, avg_loss: tensor(0.0276, grad_fn=<MeanBackward1>)\n",
      "16:27:47 - Training round: 38, worker: bob, avg_loss: tensor(0.0092, grad_fn=<MeanBackward1>)\n",
      "16:27:48 - Training round: 38, worker: charlie, avg_loss: tensor(0.1158, grad_fn=<MeanBackward1>)\n",
      "16:27:49 - Starting training round 39/40\n",
      "16:27:49 - Training round 39, calling fit on worker: alice\n",
      "16:27:50 - Training round 39, calling fit on worker: bob\n",
      "16:27:50 - Training round 39, calling fit on worker: charlie\n",
      "16:27:50 - Training round: 39, worker: charlie, avg_loss: tensor(0.1103, grad_fn=<MeanBackward1>)\n",
      "16:27:51 - Training round: 39, worker: alice, avg_loss: tensor(0.1845, grad_fn=<MeanBackward1>)\n",
      "16:27:53 - Training round: 39, worker: bob, avg_loss: tensor(0.0209, grad_fn=<MeanBackward1>)\n",
      "16:27:54 - Starting training round 40/40\n",
      "16:27:54 - Training round 40, calling fit on worker: alice\n",
      "16:27:54 - Training round 40, calling fit on worker: bob\n",
      "16:27:54 - Training round 40, calling fit on worker: charlie\n",
      "16:27:54 - Training round: 40, worker: alice, avg_loss: tensor(0.0379, grad_fn=<MeanBackward1>)\n",
      "16:27:56 - Training round: 40, worker: charlie, avg_loss: tensor(0.1584, grad_fn=<MeanBackward1>)\n",
      "16:27:57 - Training round: 40, worker: bob, avg_loss: tensor(0.0029, grad_fn=<MeanBackward1>)\n",
      "16:28:01 - alice: Prediction hist.: [1047. 1179. 1371. 1971.  969.  419.  864.  860.  583.  737.]\n",
      "16:28:01 - alice: Percentage numbers 0-3: 0.5568\n",
      "16:28:01 - alice: Percentage numbers 4-6: 0.2252\n",
      "16:28:01 - alice: Percentage numbers 7-9: 0.218\n",
      "16:28:01 - alice: Test set: Average loss: 0.0036, Accuracy: 8460/10000 (84.60)\n",
      "16:28:03 - bob: Prediction hist.: [ 825. 1121.  927.  605. 1782. 1870. 1142.  995.  557.  176.]\n",
      "16:28:03 - bob: Percentage numbers 0-3: 0.3478\n",
      "16:28:03 - bob: Percentage numbers 4-6: 0.4794\n",
      "16:28:03 - bob: Percentage numbers 7-9: 0.1728\n",
      "16:28:03 - bob: Test set: Average loss: 0.0044, Accuracy: 7902/10000 (79.02)\n",
      "16:28:06 - charlie: Prediction hist.: [ 752.  867.  532.  477.  176.  473.  837. 1168. 2342. 2376.]\n",
      "16:28:06 - charlie: Percentage numbers 0-3: 0.2628\n",
      "16:28:06 - charlie: Percentage numbers 4-6: 0.1486\n",
      "16:28:06 - charlie: Percentage numbers 7-9: 0.5886\n",
      "16:28:06 - charlie: Test set: Average loss: 0.0065, Accuracy: 7008/10000 (70.08)\n",
      "16:28:08 - Target histogram: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
      "16:28:08 - Federated model: Prediction hist.: [ 991. 1134. 1009.  963.  983.  925.  984. 1031. 1005.  975.]\n",
      "16:28:08 - Federated model: Percentage numbers 0-3: 0.4097\n",
      "16:28:08 - Federated model: Percentage numbers 4-6: 0.2892\n",
      "16:28:08 - Federated model: Percentage numbers 7-9: 0.3011\n",
      "16:28:08 - Federated model: Test set: Average loss: 0.0010, Accuracy: 9613/10000 (96.13)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = args.lr\n",
    "\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    global traced_model\n",
    "    \n",
    "    logger.info(\"Starting training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    # For each of the workers we ask the model to train with their part of their data\n",
    "    # in a async fashion and then we gather the results\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    # Run evaluation every 10 rounds\n",
    "    run_evaluation = (curr_round) % 10 == 1 or curr_round == args.training_rounds\n",
    "    if run_evaluation:\n",
    "        for worker_id, worker_model, _ in results:\n",
    "                rwc.evaluate_model_on_worker(\n",
    "                    model_identifier=worker_id,\n",
    "                    worker=testing,\n",
    "                    dataset_key=\"mnist_testing\",\n",
    "                    model=worker_model,\n",
    "                    nr_bins=10,\n",
    "                    batch_size=args.test_batch_size,\n",
    "                    print_target_hist=False,\n",
    "                )\n",
    "    \n",
    "    # Store models and loss_values for each worker\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    # Average model\n",
    "    avg_model = utils.federated_avg(models)\n",
    "    if run_evaluation:\n",
    "        rwc.evaluate_model_on_worker(\n",
    "                model_identifier=\"Federated model\",\n",
    "                worker=testing,\n",
    "                dataset_key=\"mnist_testing\",\n",
    "                model=traced_model,\n",
    "                nr_bins=10,\n",
    "                batch_size=args.test_batch_size,\n",
    "                print_target_hist=True,\n",
    "            )\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.98 * learning_rate, args.lr * 0.01)\n",
    "    \n",
    "    # Use averaged model in the next round\n",
    "    traced_model = avg_model\n",
    "\n",
    "# Save model\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40 rounds of training we achieve an accuracy > 95% on the entire testing dataset. \n",
    "This is impressing, given that no worker has access to more than 4 digits!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
